{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lf4jIBR38Lf",
        "outputId": "10012fb3-9307-4d1c-dfea-966f25bd3c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBFt1Ysh3xRY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Read the image\n",
        "I = Image.open('6.png')\n",
        "\n",
        "# Get the size of the image\n",
        "cols, rows = I.size\n",
        "channels = len(I.getbands())\n",
        "\n",
        "# Define the size of each patch\n",
        "patchSize = 100\n",
        "\n",
        "# Create a new folder to store the patches\n",
        "folderName = 'patches'\n",
        "if not os.path.exists(folderName):\n",
        "    os.makedirs(folderName)\n",
        "\n",
        "# Initialize a counter for naming patches\n",
        "patchCount = 1\n",
        "\n",
        "# Loop over the image in blocks of 20x20\n",
        "for i in range(0, rows, patchSize):\n",
        "    for j in range(0, cols, patchSize):\n",
        "        # Extract the patch\n",
        "        if (i+patchSize <= rows) and (j+patchSize <= cols):\n",
        "            patch = I.crop((j, i, j+patchSize, i+patchSize))\n",
        "            # Save the patch\n",
        "            patch.save(f'{folderName}/patch_{patchCount}.png')\n",
        "            patchCount += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Read the image\n",
        "I = cv2.imread('6.png')\n",
        "\n",
        "# Get the size of the image\n",
        "rows, cols, channels = I.shape\n",
        "\n",
        "# Define the size of each patch\n",
        "patchSize = 100  # You can change this value to any other\n",
        "\n",
        "# Create a new folder to store the patches\n",
        "folderName = 'patches'\n",
        "os.makedirs(folderName, exist_ok=True)\n",
        "\n",
        "# Initialize a counter for naming patches\n",
        "patchCount = 1\n",
        "\n",
        "# Loop over the image in blocks of patchSize x patchSize\n",
        "for i in range(0, rows, patchSize):\n",
        "    for j in range(0, cols, patchSize):\n",
        "        # Determine the patch dimensions, adjusting for the edge cases\n",
        "        rowEnd = min(i+patchSize, rows)\n",
        "        colEnd = min(j+patchSize, cols)\n",
        "\n",
        "        # Extract the patch\n",
        "        patch = I[i:rowEnd, j:colEnd, :]\n",
        "\n",
        "        # Save the patch\n",
        "        patchFileName = os.path.join(folderName, f'patch_{patchCount}.png')\n",
        "        cv2.imwrite(patchFileName, patch)\n",
        "        patchCount += 1\n"
      ],
      "metadata": {
        "id": "tNlegxfGawqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "  layers.InputLayer(input_shape=(384, 384, 3)),\n",
        "  layers.Conv2D(64, (1, 1), activation='relu', padding='same'),\n",
        "  layers.MaxPooling2D(pool_size=(8, 8)),\n",
        "  layers.Conv2D(64, (1, 1), activation='relu', padding='same'),\n",
        "  layers.MaxPooling2D(pool_size=(8, 8)),\n",
        "  layers.Conv2D(128, (1, 1), activation='relu', padding='same'),\n",
        "  layers.Conv2D(64, (1, 1), activation='relu', padding='same'),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Conv2D(3, (1, 1), activation='relu', padding='same'),\n",
        "  layers.GlobalAveragePooling2D(),\n",
        "  layers.Dense(3)\n",
        "])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qefyaCCm4JM1",
        "outputId": "c130635e-f101-4908-ad00-2b6d085c25ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 384, 384, 64)      256       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 48, 48, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 48, 48, 64)        4160      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 6, 6, 128)         8320      \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 64)          8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6, 6, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 6, 6, 3)           195       \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 3)                 0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 12        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21199 (82.81 KB)\n",
            "Trainable params: 21199 (82.81 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square((y_pred - y_true) / y_true))\n",
        "\n",
        "optimizer = tf.optimizers.AdamW(learning_rate=2e-3, weight_decay=5e-5)\n",
        "\n",
        "model.compile(optimizer= optimizer, loss= custom_loss)\n",
        "\n",
        "class CyclicLR(callbacks.Callback):\n",
        "    def _init_(self, max_lr, min_lr, half_cycle):\n",
        "        self.max_lr = max_lr\n",
        "        self.min_lr = min_lr\n",
        "        self.half_cycle = half_cycle\n",
        "        self.iteration = 0\n",
        "        super(CyclicLR, self)._init_()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        cycle = tf.floor(1 + self.iteration / (2 * self.half_cycle))\n",
        "        x = tf.abs(self.iteration / self.half_cycle - 2 * cycle + 1)\n",
        "        new_lr = self.min_lr + (self.max_lr - self.min_lr) * tf.maximum(0, (1 - x))\n",
        "        self.iteration += 1\n",
        "        self.model.optimizer.learning_rate.assign(new_lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "paWvzr0ZXXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_weights('/content/light_esti.h5')"
      ],
      "metadata": {
        "id": "z8cTnEvfXpqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "rWhHKJ6MY1c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_folder = '/content/patches'\n",
        "\n",
        "# Get the list of image file names in the test folder\n",
        "test_files = os.listdir(test_folder)\n",
        "\n",
        "# Iterate over each image file, load it, preprocess it, and make predictions\n",
        "predictions_list = []\n",
        "for filename in test_files:\n",
        "    img_path = os.path.join(test_folder, filename)\n",
        "    img = image.load_img(img_path, target_size=(384, 384))  # Assuming the same target size as during training\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Normalize pixel values\n",
        "    prediction = model.predict(img_array)\n",
        "    predictions_list.append(prediction)\n",
        "\n",
        "# Convert predictions list to a numpy array\n",
        "predictions_array = np.array(predictions_list)\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", predictions_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655M9xtZ4X8v",
        "outputId": "b21e2a59-69e4-4e92-b672-03674633f8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Predictions: [[[0.32864934 0.8353279  0.7228495 ]]\n",
            "\n",
            " [[0.19633311 0.5102718  0.3611666 ]]\n",
            "\n",
            " [[0.26878834 0.7577553  0.6414897 ]]\n",
            "\n",
            " [[0.43813178 1.2524183  1.2156649 ]]\n",
            "\n",
            " [[0.18323395 0.4661094  0.31070158]]\n",
            "\n",
            " [[0.18535125 0.47330737 0.31920677]]\n",
            "\n",
            " [[0.40771568 1.1369725  1.0868139 ]]\n",
            "\n",
            " [[0.18627934 0.47602332 0.32235834]]\n",
            "\n",
            " [[0.20120794 0.5264852  0.3796039 ]]\n",
            "\n",
            " [[0.42861867 1.2372379  1.1956025 ]]\n",
            "\n",
            " [[0.3126375  0.88466233 0.7890241 ]]\n",
            "\n",
            " [[0.44773236 1.2988029  1.265953  ]]\n",
            "\n",
            " [[0.25989804 0.7283428  0.6080163 ]]\n",
            "\n",
            " [[0.38249272 1.0883961  1.0255675 ]]\n",
            "\n",
            " [[0.26754287 0.75380224 0.6369622 ]]\n",
            "\n",
            " [[0.34086278 0.69418645 0.5347926 ]]\n",
            "\n",
            " [[0.288665   0.792402   0.6811767 ]]\n",
            "\n",
            " [[0.21995993 0.59489    0.45636442]]\n",
            "\n",
            " [[0.45041135 1.311177   1.2794352 ]]\n",
            "\n",
            " [[0.37075368 0.791871   0.65468717]]\n",
            "\n",
            " [[0.28555366 0.80827326 0.69982857]]\n",
            "\n",
            " [[0.25735414 0.72063017 0.5991185 ]]\n",
            "\n",
            " [[0.33718595 0.9002368  0.80844516]]\n",
            "\n",
            " [[0.43721685 1.2662542  1.2285273 ]]\n",
            "\n",
            " [[0.28021044 0.79383266 0.6828407 ]]\n",
            "\n",
            " [[0.1872297  0.47880435 0.32558542]]\n",
            "\n",
            " [[0.24803148 0.6800145  0.55456644]]\n",
            "\n",
            " [[0.6615247  0.570212   0.2340504 ]]\n",
            "\n",
            " [[0.39165694 1.1209415  1.0622257 ]]\n",
            "\n",
            " [[0.2924767  0.8330296  0.72768617]]\n",
            "\n",
            " [[0.2770321  0.7839515  0.671487  ]]\n",
            "\n",
            " [[0.42923996 1.244684   1.2031547 ]]\n",
            "\n",
            " [[0.39201182 1.0601552  0.9858093 ]]\n",
            "\n",
            " [[0.18486086 0.47187233 0.31754157]]\n",
            "\n",
            " [[0.36169165 1.0412772  0.9682325 ]]\n",
            "\n",
            " [[0.22092311 0.6027359  0.4644967 ]]\n",
            "\n",
            " [[0.18795802 0.4809357  0.3280586 ]]\n",
            "\n",
            " [[0.4430697  1.2707074  1.236145  ]]\n",
            "\n",
            " [[0.22002947 0.59911096 0.4604857 ]]\n",
            "\n",
            " [[0.4098161  1.1897936  1.1390834 ]]\n",
            "\n",
            " [[0.2746051  0.77598864 0.6624133 ]]\n",
            "\n",
            " [[0.2477223  0.68952787 0.5635915 ]]\n",
            "\n",
            " [[0.36523682 1.0558674  0.98434794]]\n",
            "\n",
            " [[0.18363367 0.46828115 0.31337443]]\n",
            "\n",
            " [[0.2000107  0.521863   0.3744566 ]]\n",
            "\n",
            " [[0.18598595 0.4751647  0.32136205]]\n",
            "\n",
            " [[0.22771111 0.62378496 0.4886928 ]]\n",
            "\n",
            " [[0.18743399 0.4794105  0.3262872 ]]\n",
            "\n",
            " [[0.28230315 0.7952588  0.68540376]]\n",
            "\n",
            " [[0.26929307 0.75467587 0.63879716]]\n",
            "\n",
            " [[0.24457578 0.67991793 0.55251795]]\n",
            "\n",
            " [[0.31864896 0.9024772  0.8096531 ]]\n",
            "\n",
            " [[0.26678696 0.7515073  0.63431513]]\n",
            "\n",
            " [[0.3146276  0.90150976 0.8064425 ]]\n",
            "\n",
            " [[0.19036356 0.48805308 0.33630252]]\n",
            "\n",
            " [[0.439626   1.2581592  1.2220619 ]]\n",
            "\n",
            " [[0.1877416  0.48030236 0.3273237 ]]\n",
            "\n",
            " [[0.2162423  0.5805409  0.44038475]]\n",
            "\n",
            " [[0.34840986 0.96812946 0.8820432 ]]\n",
            "\n",
            " [[0.40461004 1.0503359  0.9643477 ]]\n",
            "\n",
            " [[0.28760946 0.8176033  0.71001434]]\n",
            "\n",
            " [[0.45937607 0.6318084  0.40465   ]]\n",
            "\n",
            " [[0.19612955 0.5069401  0.35782954]]\n",
            "\n",
            " [[0.34623557 0.75731283 0.6196626 ]]\n",
            "\n",
            " [[0.19903427 0.5180783  0.37024418]]\n",
            "\n",
            " [[0.32143414 0.8118902  0.69364405]]\n",
            "\n",
            " [[0.27839196 0.78821576 0.67638004]]\n",
            "\n",
            " [[0.34404197 0.9926421  0.9112142 ]]\n",
            "\n",
            " [[0.25158298 0.7019803  0.5778179 ]]\n",
            "\n",
            " [[0.21443836 0.5795412  0.43839738]]\n",
            "\n",
            " [[0.37098613 1.0659405  0.9973419 ]]\n",
            "\n",
            " [[0.31321955 0.898779   0.8030051 ]]\n",
            "\n",
            " [[0.41662583 1.0871893  1.0087081 ]]\n",
            "\n",
            " [[0.2793733  0.7913367  0.6799533 ]]\n",
            "\n",
            " [[0.43899864 1.2751362  1.2381247 ]]\n",
            "\n",
            " [[0.24548654 0.66880596 0.54228723]]\n",
            "\n",
            " [[0.23186566 0.5430417  0.38834   ]]\n",
            "\n",
            " [[0.23728745 0.64401174 0.5136711 ]]\n",
            "\n",
            " [[0.29467133 0.58977276 0.41844413]]\n",
            "\n",
            " [[0.27281022 0.7705837  0.6561709 ]]\n",
            "\n",
            " [[0.4312975  1.2379076  1.1977658 ]]\n",
            "\n",
            " [[0.2868392  0.8151444  0.70720065]]\n",
            "\n",
            " [[0.35716945 0.90017384 0.7964206 ]]\n",
            "\n",
            " [[0.28964224 0.8239042  0.71725774]]\n",
            "\n",
            " [[0.3264373  0.9279767  0.838719  ]]\n",
            "\n",
            " [[0.33648902 0.9664808  0.8816415 ]]\n",
            "\n",
            " [[0.45170546 1.3125405  1.281486  ]]\n",
            "\n",
            " [[0.37026513 1.0719638  1.0027587 ]]\n",
            "\n",
            " [[0.18696429 0.4780277  0.3246842 ]]\n",
            "\n",
            " [[0.43402973 0.804081   0.63865405]]\n",
            "\n",
            " [[0.18883952 0.48351526 0.33105192]]\n",
            "\n",
            " [[0.37402108 1.01962    0.95142066]]\n",
            "\n",
            " [[0.25462726 0.7118741  0.58910817]]\n",
            "\n",
            " [[0.3206684  0.9221743  0.8298439 ]]\n",
            "\n",
            " [[0.31261343 0.89438426 0.7984122 ]]\n",
            "\n",
            " [[0.2385833  0.65440154 0.5244518 ]]\n",
            "\n",
            " [[0.42032957 1.1796212  1.1351941 ]]\n",
            "\n",
            " [[0.49234658 0.6886751  0.46315327]]\n",
            "\n",
            " [[0.27662206 0.78284645 0.6701862 ]]\n",
            "\n",
            " [[0.22454874 0.61253774 0.47602704]]\n",
            "\n",
            " [[0.22458051 0.6143848  0.47783116]]\n",
            "\n",
            " [[0.39161026 1.0315633  0.95260304]]\n",
            "\n",
            " [[0.24417579 0.6758782  0.548385  ]]\n",
            "\n",
            " [[0.43406624 1.2313371  1.1871912 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import savemat\n",
        "# Save predictions as a .mat file\n",
        "savemat('/content/predictions.mat', {'predictions': predictions_array})"
      ],
      "metadata": {
        "id": "AudB8hnsZnZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}